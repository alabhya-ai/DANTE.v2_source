{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dab25d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from model import InsiderClassifier, LSTM_Encoder, CNN_Classifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4da1c0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# üíæ DATASET SETUP (Re-using the structure defined previously)\n",
    "# ==============================================================================\n",
    "\n",
    "class InsiderThreatDataset(Dataset):\n",
    "    def __init__(self, X_path, y_path):\n",
    "        self.X = pd.read_pickle(X_path)\n",
    "        self.y = pd.read_pickle(y_path)\n",
    "        \n",
    "        # Convert to Tensors: X must be Long (for nn.Embedding input), y must be Float\n",
    "        self.X = torch.tensor(self.X.tolist(), dtype=torch.long)\n",
    "        # Unsqueeze(1) makes labels (N, 1) for standard binary classification\n",
    "        self.y = torch.tensor(self.y.values.astype(float), dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8de0594",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ‚öôÔ∏è CONFIGURATION VARIABLES (Change these for your specific environment)\n",
    "# ==============================================================================\n",
    "\n",
    "# --- A. DEVICE / SINGLE-GPU CONFIG ---\n",
    "# 'cpu', 'cuda', or 'cuda:0'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# --- B. DDP (Distributed Data Parallel) CONFIG ---\n",
    "# Set to True to enable DDP for multi-GPU/multi-node training\n",
    "USE_DDP = False\n",
    "# DDP parameters (only relevant if USE_DDP is True)\n",
    "RANK = 0         # Rank of the current process (0 to WORLD_SIZE - 1)\n",
    "WORLD_SIZE = 1   # Total number of participating GPUs/processes\n",
    "BACKEND = 'nccl' # Communication backend (usually 'nccl' for GPUs)\n",
    "INIT_METHOD = 'env://' # How processes find each other (e.g., environment variables)\n",
    "\n",
    "# --- C. HYPERPARAMETERS & DATA LOADING ---\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4 # extremely small learning rate\n",
    "# Data paths (Assuming X.pkl contains padded action_id sequences, y.pkl contains labels)\n",
    "X_PATH = 'X_train.pkl'\n",
    "Y_PATH = 'y_train.pkl'\n",
    "NUM_WORKERS = 0 # How many subprocesses to use for data loading\n",
    "\n",
    "# ==============================================================================\n",
    "# üöÄ MAIN TRAINING FUNCTION (UPDATED)\n",
    "# ==============================================================================\n",
    "\n",
    "def train_cnn_classifier(EPOCHS=10, OUTPUT_FILENAME='model.pkl', LSTM_CHECKPOINT_PATH='./kk'):\n",
    "# ------------------------------------\n",
    "    # 1. DDP and Device Initialization\n",
    "    # ------------------------------------\n",
    "    if USE_DDP:\n",
    "        # Initialize the distributed process group\n",
    "        dist.init_process_group(BACKEND, init_method=INIT_METHOD, rank=RANK, world_size=WORLD_SIZE)\n",
    "        # Use the local rank (device ID) as the actual training device\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"]) if \"LOCAL_RANK\" in os.environ else 0\n",
    "        current_device = torch.device(f'cuda:{local_rank}')\n",
    "        print(f\"DDP: Rank {RANK} initialized on device {current_device}\")\n",
    "    else:\n",
    "        # Single-device setup\n",
    "        current_device = torch.device(DEVICE)\n",
    "        print(f\"Single Device: Initializing on device {current_device}\")\n",
    "\n",
    "    # ------------------------------------\n",
    "    # 2. Data Loading\n",
    "    # ------------------------------------\n",
    "    dataset = InsiderThreatDataset(X_PATH, Y_PATH)\n",
    "\n",
    "    if USE_DDP:\n",
    "        # Use DistributedSampler for DDP\n",
    "        sampler = DistributedSampler(dataset, num_replicas=WORLD_SIZE, rank=RANK, shuffle=True)\n",
    "        # When using DDP, DataLoader should NOT shuffle, sampler handles it\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            sampler=sampler,\n",
    "            num_workers=NUM_WORKERS\n",
    "        )\n",
    "    else:\n",
    "        # Standard DataLoader for single device\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # 3. Model, Loss, and Optimizer\n",
    "    # ------------------------------------\n",
    "\n",
    "    # Initialize the InsiderClassifier model\n",
    "    model = InsiderClassifier(\n",
    "        lstm_checkpoint=LSTM_CHECKPOINT_PATH,\n",
    "        device=current_device\n",
    "    )\n",
    "\n",
    "    # DDP wrapping (if applicable)\n",
    "    if USE_DDP:\n",
    "        model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank])\n",
    "\n",
    "    # Use standard CrossEntropyLoss (expects raw logits from CNN_Classifier)\n",
    "    # Use standard CrossEntropyLoss\n",
    "    class_weights = torch.tensor((1.0, 49.0)).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    print(f\"Starting unweighted training...\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        if USE_DDP:\n",
    "            dataloader.sampler.set_epoch(epoch)\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Lists to temporarily hold predictions/labels for the metrics calculation\n",
    "        temp_preds, temp_labels = [], []\n",
    "\n",
    "        for i, (X_batch, y_batch) in enumerate(dataloader):\n",
    "            if X_batch.shape[1] != 250:\n",
    "                print(\"actual batch shape is\", X_batch.shape)\n",
    "            X_batch = X_batch.long().to(current_device)\n",
    "            # CrossEntropyLoss expects (N,) Long tensor for labels\n",
    "            y_batch_long = y_batch.long().squeeze(1).to(current_device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scores = model(X_batch)\n",
    "            loss = criterion(scores, y_batch_long)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # --- METRICS COLLECTION ---\n",
    "            _, predicted_classes = torch.max(scores, 1)\n",
    "\n",
    "            temp_preds.extend(predicted_classes.cpu().tolist())\n",
    "            temp_labels.extend(y_batch_long.cpu().tolist())\n",
    "\n",
    "            # Print progress every 100 batches\n",
    "            if (i + 1) % 100 == 0:\n",
    "\n",
    "                batch_preds = np.array(temp_preds)\n",
    "                batch_labels = np.array(temp_labels)\n",
    "\n",
    "                # --- CALCULATE ALL METRICS ---\n",
    "                batch_accuracy = accuracy_score(batch_labels, batch_preds)\n",
    "                batch_f1 = f1_score(batch_labels, batch_preds, average='binary', zero_division=0)\n",
    "\n",
    "                # New: Calculate Recall and Precision for the Malicious (positive/Class 1) class\n",
    "                batch_recall = recall_score(batch_labels, batch_preds, average='binary', zero_division=0)\n",
    "                batch_precision = precision_score(batch_labels, batch_preds, average='binary', zero_division=0)\n",
    "\n",
    "                # Print the combined report\n",
    "                avg_loss = running_loss / 100\n",
    "                print(f\"[Epoch {epoch+1}, Batch {i+1}] \"\n",
    "                      f\"Loss: {avg_loss:.4f} | \"\n",
    "                      f\"Acc: {batch_accuracy:.4f} | \"\n",
    "                      f\"F1: {batch_f1:.4f} | \"\n",
    "                      f\"Prec: {batch_precision:.4f} | \"\n",
    "                      f\"Recall: {batch_recall:.4f}\")\n",
    "\n",
    "                # Reset counters for the next 100 batches\n",
    "                running_loss = 0.0\n",
    "                temp_preds, temp_labels = [], []\n",
    "    # ------------------------------------\n",
    "    # 5. Final Model Saving (Updated Logic)\n",
    "    # ------------------------------------\n",
    "\n",
    "    # In DDP, ensure only Rank 0 saves the model\n",
    "    if not USE_DDP or RANK == 0:\n",
    "\n",
    "        # Get the actual model state, unwrapping DDP if necessary\n",
    "        model_to_save = model.module if USE_DDP else model\n",
    "\n",
    "        # Save the final model state to 'model.pkl'\n",
    "        torch.save(model_to_save.state_dict(), OUTPUT_FILENAME)\n",
    "        print(f\"\\nTraining Complete. Final model parameters saved to: {OUTPUT_FILENAME}\")\n",
    "\n",
    "    if USE_DDP:\n",
    "        dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f270738f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_lstm_encoder(EPOCHS=2, LSTM_CHECKPOINT_PATH='./kk'):\n",
    "    # ------------------------------------\n",
    "    # 1. Device and Data Loading Setup\n",
    "    # ------------------------------------\n",
    "    current_device = torch.device(DEVICE)\n",
    "    print(f\"Starting LSTM Encoder training on device {current_device}...\")\n",
    "\n",
    "    # For the Encoder training, we don't need the y_path (malicious labels)\n",
    "    # but the InsiderThreatDataset loads it, so we'll just ignore it in the loop.\n",
    "    dataset = InsiderThreatDataset(X_PATH, Y_PATH)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # 2. Model, Loss, and Optimizer\n",
    "    # ------------------------------------\n",
    "\n",
    "    # Initialize the Encoder model and move to device\n",
    "    # Note: LSTM_Encoder is NOT wrapped in the InsiderClassifier here.\n",
    "    model = LSTM_Encoder().to(current_device)\n",
    "    model.train() # Set to training mode for dropout and decoder output\n",
    "\n",
    "    # Loss function for reconstruction (input is categorical, output is LogSoftmax)\n",
    "    # We use NLLLoss combined with F.one_hot() to handle the categorical reconstruction.\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # ------------------------------------\n",
    "    # 3. Training Loop\n",
    "    # ------------------------------------\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (X_batch, _) in enumerate(dataloader):\n",
    "            # X_batch is the padded sequence (B, S). Target must be Long tensor.\n",
    "            X_batch = X_batch.to(current_device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: the encoder returns the reconstructed sequence (LogSoftmax output)\n",
    "            reconstructed_X = model(X_batch)\n",
    "\n",
    "            # --- Calculate Loss (Categorical Reconstruction) ---\n",
    "            # 1. reconstructed_X shape: (B, S, V) [Logits]\n",
    "            # 2. X_batch shape: (B, S) [Target IDs]\n",
    "            # NLLLoss expects (N*V, C) logits and (N*V,) target IDs.\n",
    "\n",
    "            # Reshape logits to (B*S, V) and target to (B*S)\n",
    "            loss = criterion(\n",
    "                reconstructed_X.permute(0, 2, 1), # NLLLoss expects (B, V, S) input\n",
    "                X_batch.long()\n",
    "            )\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Avg Reconstruction Loss: {avg_loss:.4f}\", end='\\t')\n",
    "        print()\n",
    "\n",
    "    # ------------------------------------\n",
    "    # 4. Save the Checkpoint\n",
    "    # ------------------------------------\n",
    "    # Save the final state dictionary to the required path\n",
    "    torch.save(model.state_dict(), f'{LSTM_CHECKPOINT_PATH}')\n",
    "    print(f\"\\nLSTM Encoder training complete. Checkpoint saved to: {LSTM_CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdb77a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Start Training\n",
    "\n",
    "train_lstm_encoder(EPOCHS=2, LSTM_CHECKPOINT_PATH = 'kk (1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf56f33",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_cnn_classifier(EPOCHS=10, OUTPUT_FILENAME = 'model.pkl', LSTM_CHECKPOINT_PATH = 'kk (1)')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
