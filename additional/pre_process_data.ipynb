{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "from https://github.com/aiforsec/InsiderThreat\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "efDzjBNxbORx",
        "outputId": "076effb0-9924-42d4-9a90-b6593275aad6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nif __name__ == \"__main__\":\\n    if len(sys.argv) != 3:\\n        print(\"Usage:\", sys.argv[0], \"[answer directory] [dataset directory]\")\\n        print(\"Warning: may cost over 30 hours to run all steps for the dataset over 20G\")\\n        print(\"hint: comment out the pre_runned process\")'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import re\n",
        "from gensim.models import TfidfModel, nmf\n",
        "from gensim.corpora import Dictionary as Dict\n",
        "from gensim.models.ldamulticore import LdaModel\n",
        "from multiprocessing import Pool\n",
        "from functools import partial\n",
        "\n",
        "CHUNK_SIZE = 10000\n",
        "TOPIC_NUM = 100\n",
        "ALLFILES = ['logon.csv','device.csv', 'email.csv', 'file.csv', 'http.csv']\n",
        "CONTENT_FILES = ['email.csv', 'file.csv', 'http.csv']\n",
        "\n",
        "def chunk_iterator(filename):\n",
        "    for chunk in pd.read_csv(filename, chunksize=CHUNK_SIZE):\n",
        "        for document in chunk['content'].str.lower().str.split().values:\n",
        "            yield document\n",
        "\n",
        "\n",
        "def tfidf_iterator(filenames, dictionary):\n",
        "    for filename in filenames:\n",
        "        for chunk in pd.read_csv(dataset_dir /filename, chunksize=CHUNK_SIZE):\n",
        "            for document in chunk['content'].str.lower().str.split().values:\n",
        "                yield dictionary.doc2bow(document)\n",
        "\n",
        "\n",
        "def nmf_iterator(filenames, dictionary, tfidf):\n",
        "    for filename in filenames:\n",
        "        for chunk in pd.read_csv(dataset_dir /filename, chunksize=CHUNK_SIZE): #added datset_dir\n",
        "            for document in chunk['content'].str.lower().str.split().values:\n",
        "                yield tfidf[dictionary.doc2bow(document)]\n",
        "\n",
        "\n",
        "# referenced\n",
        "# https://stackoverflow.com/questions/26784164/pandas-multiprocessing-apply/53135031#53135031\n",
        "def parallelize(data, func, num_of_processes=8):\n",
        "    data_split = np.array_split(data, num_of_processes)\n",
        "    pool = Pool(num_of_processes)\n",
        "    data = pd.concat(pool.map(func, data_split))\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return data\n",
        "\n",
        "\n",
        "def process_content(filename, chunk_size=CHUNK_SIZE):\n",
        "    model = LdaModel.load((output_dir / 'lda_model.pkl').as_posix())\n",
        "    temp_dict = Dict.load((output_dir / 'dict.pkl').as_posix())\n",
        "    out_file = output_dir / (filename.stem + '_lda.csv')\n",
        "    # touch one while not exist\n",
        "    if not out_file.is_file():\n",
        "        Path(out_file).touch()\n",
        "\n",
        "    for chunk in pd.read_csv(filename, usecols=['id', 'content'], chunksize=chunk_size):\n",
        "        chunk['content'] = chunk['content'].str.lower().str.split() \\\n",
        "            .apply(lambda doc: model[temp_dict.doc2bow(doc)])\n",
        "\n",
        "        chunk.to_csv(out_file, mode='a')\n",
        "\n",
        "\n",
        "def pre_process_logon(path):\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y %H:%M:%S')\n",
        "    df['day'] = pd.to_datetime(df['date'], format='%m/%d/%Y %H:%M:%S').dt.floor('D')\n",
        "    self_pc = df \\\n",
        "        .groupby(['user', 'date', 'pc']).size().to_frame('count') \\\n",
        "        .reset_index().sort_values('count', ascending=False) \\\n",
        "        .drop_duplicates(subset=['user', 'date']) \\\n",
        "        .drop(columns=['count']).sort_values(['user', 'date']) \\\n",
        "        .groupby('user').pc.agg(pd.Series.mode).rename('self_pc')\n",
        "    df = df.merge(self_pc.to_frame(), left_on='user', right_on='user')\n",
        "    df['is_usual_pc'] = df['self_pc'] == df['pc']\n",
        "\n",
        "    is_work_time = (8 <= df.date.dt.hour) & (df.date.dt.hour < 17)\n",
        "    df['is_work_time'] = is_work_time\n",
        "\n",
        "    df['subtype'] = df['activity']\n",
        "    df[['id', 'date', 'user', 'is_usual_pc', 'is_work_time', 'subtype']].to_csv(output_dir / 'logon_preprocessed.csv')\n",
        "    return self_pc.to_frame()\n",
        "\n",
        "\n",
        "def pre_process_device(path):\n",
        "    df = pd.read_csv(path)\n",
        "    df['date'] = pd.to_datetime(df.date, format='%m/%d/%Y %H:%M:%S')\n",
        "    df = df.merge(self_pc, left_on='user', right_on='user', )\n",
        "    df['is_usual_pc'] = df['self_pc'] == df['pc']\n",
        "\n",
        "    is_work_time = (8 <= df.date.dt.hour) & (df.date.dt.hour < 17)\n",
        "    df['is_work_time'] = is_work_time\n",
        "\n",
        "    df['subtype'] = df['activity']\n",
        "    df[['id', 'date', 'user', 'is_usual_pc', 'is_work_time', 'subtype']].to_csv(\n",
        "        output_dir / f'device_preprocessed.csv')\n",
        "\n",
        "\n",
        "def pre_process_file(path):\n",
        "    df = pd.read_csv(path, usecols=['id', 'date', 'user', 'pc', 'filename'])\n",
        "    df['date'] = pd.to_datetime(df.date, format='%m/%d/%Y %H:%M:%S')\n",
        "\n",
        "    df = df.merge(self_pc, left_on='user', right_on='user', )\n",
        "    df['is_usual_pc'] = df['self_pc'] == df['pc']\n",
        "\n",
        "    is_work_time = (8 <= df.date.dt.hour) & (df.date.dt.hour < 17)\n",
        "    df['is_work_time'] = is_work_time\n",
        "\n",
        "    file_extensions = df.filename.str[-4:]\n",
        "    df['subtype'] = file_extensions\n",
        "    df[['id', 'date', 'user', 'is_usual_pc', 'is_work_time', 'subtype']].to_csv(\n",
        "        output_dir / f'file_preprocessed.csv')\n",
        "\n",
        "\n",
        "def pre_process_email(path):\n",
        "    df = pd.read_csv(path, usecols=['id', 'date', 'user', 'pc', 'to', 'cc', 'bcc', 'from'])\n",
        "    df = df.fillna('') # apply instead of progress_apply\n",
        "    to_concated = df[['to', 'cc', 'bcc']].apply(lambda x: ';'.join([x.to, x.cc, x.bcc]), axis=1)\n",
        "    is_external_to = to_concated.apply(\n",
        "        lambda x: any([re.match('^.+@(.+$)', e).group(1) != 'dtaa.com' for e in x.split(';') if e != '']))\n",
        "    is_external = is_external_to | is_external_to\n",
        "    df['date'] = pd.to_datetime(df.date, format='%m/%d/%Y %H:%M:%S')\n",
        "\n",
        "    df = df.merge(self_pc, left_on='user', right_on='user', )\n",
        "    df['is_usual_pc'] = df['self_pc'] == df['pc']\n",
        "\n",
        "    is_work_time = (8 <= df.date.dt.hour) & (df.date.dt.hour < 17)\n",
        "    df['is_work_time'] = is_work_time\n",
        "\n",
        "    df['subtype'] = is_external\n",
        "    df[['id', 'date', 'user', 'is_usual_pc', 'is_work_time', 'subtype']].to_csv(\n",
        "        output_dir / f'email_preprocessed.csv')\n",
        "\n",
        "\n",
        "def pre_process_http(path):\n",
        "\n",
        "    # scenario 1\n",
        "    scenario_1_http = [\n",
        "        'actualkeylogger.com',\n",
        "        'best-spy-soft.com',\n",
        "        'dailykeylogger.com',\n",
        "        'keylogpc.com',\n",
        "        'refog.com',\n",
        "        'relytec.com',\n",
        "        'softactivity.com',\n",
        "        'spectorsoft.com',\n",
        "        'webwatchernow.com',\n",
        "        'wellresearchedreviews.com',\n",
        "        'wikileaks.org'\n",
        "    ]\n",
        "\n",
        "    # scenario 2\n",
        "    scenario_2_http = [\n",
        "        'careerbuilder.com',\n",
        "        'craiglist.org',\n",
        "        'indeed.com',\n",
        "        'job-hunt.org',\n",
        "        'jobhuntersbible.com',\n",
        "        'linkedin.com',\n",
        "        'monster.com',\n",
        "        'simplyhired.com',\n",
        "    ]\n",
        "\n",
        "\n",
        "    #scenario 3\n",
        "    scenario_3_http = [\n",
        "        '4shared.com'\n",
        "        'dropbox.com',\n",
        "        'fileserve.com',\n",
        "        'filefreak.com',\n",
        "        'filestube.com',\n",
        "        'megaupload.com',\n",
        "        'thepiratebay.org'\n",
        "    ]\n",
        "\n",
        "    first_it = True\n",
        "    mode = 'w'\n",
        "\n",
        "    for http_df in pd.read_csv(path, chunksize=CHUNK_SIZE, usecols=['id', 'date', 'user', 'pc', 'url']): # update: usecols id\n",
        "        http_df['date'] = pd.to_datetime(http_df.date, format='%m/%d/%Y %H:%M:%S')\n",
        "\n",
        "        site_names = http_df['url'].apply(lambda s: re.match(r'^https?://(www)?([0-9\\-\\w\\.]+)?.+$', s).group(2)) # update: r string\n",
        "        http_df['site_name'] = site_names\n",
        "\n",
        "        http_df['subtype'] = 0\n",
        "        http_df.loc[site_names.isin(scenario_1_http), 'subtype'] = 1\n",
        "        http_df.loc[site_names.isin(scenario_2_http), 'subtype'] = 2\n",
        "        http_df.loc[site_names.isin(scenario_3_http), 'subtype'] = 3\n",
        "\n",
        "        http_df = http_df.merge(self_pc, left_on='user', right_on='user', )\n",
        "        http_df['is_usual_pc'] = http_df['self_pc'] == http_df['pc']\n",
        "\n",
        "        is_work_time = (8 <= http_df.date.dt.hour) & (http_df.date.dt.hour < 17)\n",
        "        http_df['is_work_time'] = is_work_time\n",
        "\n",
        "        http_df.to_csv(output_dir / 'http_preprocessed.csv', header=first_it, index=False,\n",
        "                       mode=mode, columns=['id', 'date', 'user', 'is_usual_pc', 'is_work_time', 'subtype', 'site_name'])\n",
        "        first_it = False\n",
        "        mode = 'a'\n",
        "\n",
        "\n",
        "def merge_all_content():\n",
        "    df_dict = Dict(chunk_iterator(dataset_dir / 'email.csv'))\n",
        "    df_dict.add_documents(chunk_iterator(dataset_dir / 'file.csv'))\n",
        "    df_dict.add_documents(chunk_iterator(dataset_dir / 'http.csv'))\n",
        "\n",
        "    df_dict.save((output_dir / 'dict.pkl').as_posix())\n",
        "\n",
        "\n",
        "def make_tfidf_model():\n",
        "    tfidf_model = TfidfModel(\n",
        "        tfidf_iterator(CONTENT_FILES, Dict.load((output_dir / 'dict.pkl').as_posix())))\n",
        "\n",
        "    tfidf_model.save((output_dir / 'tfidf_model.pkl').as_posix())\n",
        "\n",
        "\n",
        "def make_nmf_model():\n",
        "    tfidf_model = TfidfModel.load((output_dir / 'tfidf_model.pkl').as_posix())\n",
        "    nmf_model = nmf.Nmf(\n",
        "        nmf_iterator(CONTENT_FILES, Dict.load((output_dir / 'dict.pkl').as_posix()),\n",
        "                     tfidf_model), num_topics=TOPIC_NUM)\n",
        "    nmf_model.save((output_dir / 'nmf_model.pkl').as_posix())\n",
        "\n",
        "\n",
        "\n",
        "def make_lda_model():\n",
        "    tfidf_model = TfidfModel.load((output_dir / 'tfidf_model.pkl').as_posix())\n",
        "    lda_model = LdaModel(\n",
        "        nmf_iterator(CONTENT_FILES, Dict.load((output_dir / 'dict.pkl').as_posix()),\n",
        "                     tfidf_model), num_topics=TOPIC_NUM)\n",
        "    lda_model.save((output_dir / 'lda_model.pkl').as_posix())\n",
        "\n",
        "'''\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) != 3:\n",
        "        print(\"Usage:\", sys.argv[0], \"[answer directory] [dataset directory]\")\n",
        "        print(\"Warning: may cost over 30 hours to run all steps for the dataset over 20G\")\n",
        "        print(\"hint: comment out the pre_runned process\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgIzsA4rcIu5",
        "outputId": "058f5a44-062a-46af-f4d9-6a856936343c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start to process dataset ver kernel-b48a3b43-fb95-4c5d-a504-1de52c103143.json\n",
            "logon processed\n",
            "device processed\n",
            "file processed\n",
            "email processed\n",
            "http processed\n",
            "all content file merged and saved\n"
          ]
        }
      ],
      "source": [
        "if True:\n",
        "  if True:\n",
        "    print(\"Start to process dataset ver\", sys.argv[2].split('/')[-1])\n",
        "    answers_dir = Path('/content/drive/MyDrive/ITD_Files/src/DANTE/answers')\n",
        "    dataset_dir = Path('/content/drive/MyDrive/ITD_Files/src/DANTE/r5.2')\n",
        "    main_answers_file = answers_dir / \"insiders.csv\"\n",
        "    output_dir = Path('./_output/')\n",
        "\n",
        "    if not os.path.isdir(output_dir):  # case of no _output directory\n",
        "        os.mkdir(output_dir)\n",
        "\n",
        "        self_pc = pre_process_logon(dataset_dir / 'logon.csv')\n",
        "        '''\n",
        "\n",
        "        print(\"logon processed\")\n",
        "\n",
        "        pre_process_device(dataset_dir / 'device.csv')\n",
        "        print(\"device processed\")\n",
        "\n",
        "        pre_process_file(dataset_dir / 'file.csv')\n",
        "        print(\"file processed\")\n",
        "\n",
        "        pre_process_email(dataset_dir / 'email.csv')\n",
        "        print(\"email processed\")\n",
        "\n",
        "        pre_process_http(dataset_dir / 'http.csv')\n",
        "        print(\"http processed\")\n",
        "\n",
        "        merge_all_content()\n",
        "        print(\"all content file merged and saved\")'''\n",
        "        make_tfidf_model()\n",
        "        print(\"tfidf model saved\")\n",
        "\n",
        "        make_nmf_model()\n",
        "        print(\"nmf model saved\")\n",
        "        make_lda_model()\n",
        "        print(\"lda model saved\")\n",
        "\n",
        "    #pre process all content files\n",
        "    for file in CONTENT_FILES:\n",
        "        process_content(dataset_dir / file)\n",
        "        print(file, \"content processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aVsDB1UeExV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
