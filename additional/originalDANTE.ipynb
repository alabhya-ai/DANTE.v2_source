{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training the original DANTE model in 10 epochs\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ENsXVzcjb0Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, DistributedSampler\n",
        "import torch.distributed as dist\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score"
      ],
      "metadata": {
        "id": "6bi8JQ1Qf149"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datasets"
      ],
      "metadata": {
        "id": "CFpM3wbAilmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extractXYogDANTE"
      ],
      "metadata": {
        "id": "Ncrfiotdik0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# üíæ DATASET SETUP (Re-using the structure defined previously)\n",
        "# ==============================================================================\n",
        "\n",
        "class InsiderThreatDataset(Dataset):\n",
        "    def __init__(self, X_path, y_path):\n",
        "        self.X = pd.read_pickle(X_path)\n",
        "        self.y = pd.read_pickle(y_path)\n",
        "\n",
        "        # Convert to Tensors: X must be Long (for nn.Embedding input), y must be Float\n",
        "        self.X = torch.tensor(self.X.tolist(), dtype=torch.long)\n",
        "        # Unsqueeze(1) makes labels (N, 1) for standard binary classification\n",
        "        self.y = torch.tensor(self.y.values.astype(float), dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "bJ0oRfhvf6NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Definition"
      ],
      "metadata": {
        "id": "NBuf7c3jixB3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsvR2x9yeOGS"
      },
      "outputs": [],
      "source": [
        "# This file contains the DANTE.v2 model definition (InsiderClassifier)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Model Definition:\n",
        "\n",
        "class LSTM_Encoder(nn.Module):\n",
        "    def __init__(self, padding_idx=None, input_size=201, embedding_dim=40, lstm_hidden_size=40, num_layers=3, dropout_rate=0.5):\n",
        "        super(LSTM_Encoder, self).__init__()\n",
        "\n",
        "        # Model hyperparameters/constants\n",
        "        self.input_size = input_size\n",
        "        self.lstm_hidden_size = lstm_hidden_size\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(input_size, embedding_dim, padding_idx=padding_idx)\n",
        "        lstm_input_size = embedding_dim\n",
        "\n",
        "        # One-hot encoder fallback (optional)\n",
        "        self.one_hot_encoder = F.one_hot\n",
        "\n",
        "        # Core LSTM Encoder\n",
        "        self.lstm_encoder = nn.LSTM(\n",
        "            lstm_input_size,\n",
        "            lstm_hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout_rate,\n",
        "            batch_first=True)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        # Decoder maps hidden_size back to the input vocab size (input_size)\n",
        "        self.decoder = nn.Linear(lstm_hidden_size, input_size)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        # sequence shape: (batch_size, seq_len)\n",
        "\n",
        "        # 1. Input Processing (Embedding or One-Hot)\n",
        "        if self.embedding:\n",
        "            x = self.embedding(sequence)\n",
        "        else:\n",
        "            x = self.one_hot_encoder(sequence,\n",
        "                num_classes=self.input_size).float()\n",
        "        # x shape: (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        # 2. LSTM Forward Pass\n",
        "        x, _ = self.lstm_encoder(x)\n",
        "        # x shape: (batch_size, seq_len, lstm_hidden_size)\n",
        "\n",
        "        # 3. Output for Training (Reconstruction) or Inference (Hidden State)\n",
        "        if self.training:\n",
        "            x = self.dropout(x)\n",
        "            x = self.decoder(x)\n",
        "            x = self.log_softmax(x)\n",
        "            # Output for reconstruction loss: (batch_size, seq_len, input_size)\n",
        "            return x\n",
        "        else:\n",
        "            # Output for Classifier: (batch_size, seq_len, lstm_hidden_size)\n",
        "            return x\n",
        "\n",
        "\n",
        "class CNN_Classifier(nn.Module):\n",
        "    def __init__(self, seq_length=200, lstm_hidden_size=40):\n",
        "        super(CNN_Classifier, self).__init__()\n",
        "\n",
        "        self.seq_length = seq_length\n",
        "        self.lstm_hidden_size = lstm_hidden_size\n",
        "\n",
        "        final_seq_dim = self.seq_length // 4\n",
        "        final_hidden_dim = self.lstm_hidden_size // 4\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n",
        "        self.maxpool1 = nn.MaxPool2d(2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
        "        self.maxpool2 = nn.MaxPool2d(2, stride=2)\n",
        "\n",
        "        # Calculate the required linear input size dynamically\n",
        "        linear_input_size = 64 * final_seq_dim * final_hidden_dim\n",
        "\n",
        "        self.flatten = lambda x: x.view(x.size(0),-1)\n",
        "        self.linear = nn.Linear(linear_input_size, 2) # Output 2 classes (malicious/not malicious)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class InsiderClassifier(nn.Module):\n",
        "    def __init__(self, lstm_checkpoint, device='cuda'):\n",
        "        super(InsiderClassifier, self).__init__()\n",
        "\n",
        "        # Assuming similar architecture as in the original code:\n",
        "        lstm_hidden_size = 40\n",
        "        seq_length = 200 # Adjusted (max_length=250)\n",
        "\n",
        "        self.lstm_encoder = LSTM_Encoder(lstm_hidden_size=lstm_hidden_size)\n",
        "        self.lstm_encoder.requires_grad = False\n",
        "        self.lstm_encoder.eval()\n",
        "        self.load_encoder(lstm_checkpoint, device=device) # Pass device here!\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.cnn_classifier = CNN_Classifier(seq_length=seq_length, lstm_hidden_size=lstm_hidden_size)\n",
        "\n",
        "        # Move the entire model to the correct device upon initialization\n",
        "        self.to(device)\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        # Only the CNN classifier is trained, the encoder stays in eval mode\n",
        "        self.training = mode\n",
        "        self.sigmoid.train(mode)\n",
        "        self.cnn_classifier.train(mode)\n",
        "\n",
        "        # Crucially, the encoder MUST remain in evaluation mode and have grad disabled.\n",
        "        self.lstm_encoder.eval()\n",
        "        self.lstm_encoder.requires_grad = False\n",
        "        return self\n",
        "\n",
        "    def load_encoder(self, checkpoint, device):\n",
        "        # Map location ensures the checkpoint is loaded correctly regardless of current device\n",
        "        self.lstm_encoder.load_state_dict(\n",
        "            torch.load(\n",
        "                checkpoint,\n",
        "                map_location=torch.device(device)),\n",
        "            strict=True\n",
        "            )\n",
        "        # Move the encoder to the target device after loading its state\n",
        "        self.lstm_encoder.to(device)\n",
        "        return self\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure input data is on the same device as the model\n",
        "        device = next(self.parameters()).device\n",
        "        x = x.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # The encoder is on the correct device due to __init__ and load_encoder\n",
        "            hidden_state = self.lstm_encoder(x)\n",
        "            hidden_state = self.sigmoid(hidden_state)\n",
        "\n",
        "        # hidden_state shape: (batch_size, seq_len, lstm_hidden_size)\n",
        "        # CNN expects (N, C, H, W). We add the channel dimension (C=1)\n",
        "        scores = self.cnn_classifier(hidden_state.unsqueeze(1))\n",
        "\n",
        "        return scores\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# ‚öôÔ∏è CONFIGURATION VARIABLES (Change these for your specific environment)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- A. DEVICE / SINGLE-GPU CONFIG ---\n",
        "# 'cpu', 'cuda', or 'cuda:0'\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "# --- B. DDP (Distributed Data Parallel) CONFIG ---\n",
        "# Set to True to enable DDP for multi-GPU/multi-node training\n",
        "USE_DDP = False\n",
        "# DDP parameters (only relevant if USE_DDP is True)\n",
        "RANK = 0         # Rank of the current process (0 to WORLD_SIZE - 1)\n",
        "WORLD_SIZE = 1   # Total number of participating GPUs/processes\n",
        "BACKEND = 'nccl' # Communication backend (usually 'nccl' for GPUs)\n",
        "INIT_METHOD = 'env://' # How processes find each other (e.g., environment variables)\n",
        "\n",
        "# --- C. HYPERPARAMETERS & DATA LOADING ---\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 1e-3 # default learning rate for torch.optim.Adam\n",
        "# Data paths (Assuming X.pkl contains padded action_id sequences, y.pkl contains labels)\n",
        "X_PATH = 'X_train_ogDANTE.pkl'\n",
        "Y_PATH = 'y_train_ogDANTE.pkl'\n",
        "NUM_WORKERS = 0 # How many subprocesses to use for data loading\n",
        "\n",
        "# ==============================================================================\n",
        "# üöÄ MAIN TRAINING FUNCTION (UPDATED)\n",
        "# ==============================================================================\n",
        "\n",
        "def train_cnn_classifier(EPOCHS=10, OUTPUT_FILENAME='ogDANTE_logsoft.pkl', LSTM_CHECKPOINT_PATH='kkogDANTE'):\n",
        "# ------------------------------------\n",
        "    # 1. DDP and Device Initialization\n",
        "    # ------------------------------------\n",
        "    if USE_DDP:\n",
        "        # Initialize the distributed process group\n",
        "        dist.init_process_group(BACKEND, init_method=INIT_METHOD, rank=RANK, world_size=WORLD_SIZE)\n",
        "        # Use the local rank (device ID) as the actual training device\n",
        "        local_rank = int(os.environ[\"LOCAL_RANK\"]) if \"LOCAL_RANK\" in os.environ else 0\n",
        "        current_device = torch.device(f'cuda:{local_rank}')\n",
        "        print(f\"DDP: Rank {RANK} initialized on device {current_device}\")\n",
        "    else:\n",
        "        # Single-device setup\n",
        "        current_device = torch.device(DEVICE)\n",
        "        print(f\"Single Device: Initializing on device {current_device}\")\n",
        "\n",
        "    # ------------------------------------\n",
        "    # 2. Data Loading\n",
        "    # ------------------------------------\n",
        "    dataset = InsiderThreatDataset(X_PATH, Y_PATH)\n",
        "\n",
        "    if USE_DDP:\n",
        "        # Use DistributedSampler for DDP\n",
        "        sampler = DistributedSampler(dataset, num_replicas=WORLD_SIZE, rank=RANK, shuffle=True)\n",
        "        # When using DDP, DataLoader should NOT shuffle, sampler handles it\n",
        "        dataloader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            sampler=sampler,\n",
        "            num_workers=NUM_WORKERS\n",
        "        )\n",
        "    else:\n",
        "        # Standard DataLoader for single device\n",
        "        dataloader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=True,\n",
        "            num_workers=NUM_WORKERS\n",
        "        )\n",
        "\n",
        "    # ------------------------------------\n",
        "    # 3. Model, Loss, and Optimizer\n",
        "    # ------------------------------------\n",
        "\n",
        "    # Initialize the InsiderClassifier model\n",
        "    model = InsiderClassifier(\n",
        "        lstm_checkpoint=LSTM_CHECKPOINT_PATH,\n",
        "        device=current_device\n",
        "    )\n",
        "\n",
        "    # DDP wrapping (if applicable)\n",
        "    if USE_DDP:\n",
        "        model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank])\n",
        "\n",
        "    criterion = nn.NLLLoss() # original DANTE used this one\n",
        "    logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters()) # original one used default lr\n",
        "\n",
        "    print(f\"Starting unweighted training...\")\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        if USE_DDP:\n",
        "            dataloader.sampler.set_epoch(epoch)\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Lists to temporarily hold predictions/labels for the metrics calculation\n",
        "        temp_preds, temp_labels = [], []\n",
        "\n",
        "        for i, (X_batch, y_batch) in enumerate(dataloader):\n",
        "            if X_batch.shape[1] != 200:\n",
        "                print(\"actual batch shape is\", X_batch.shape)\n",
        "            X_batch = X_batch.long().to(current_device)\n",
        "            # CrossEntropyLoss expects (N,) Long tensor for labels\n",
        "            y_batch_long = y_batch.long().squeeze(1).to(current_device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            scores = model(X_batch)\n",
        "            loss = criterion(logsoftmax(scores), y_batch_long) # logsoftmax used to solve exploding gradients\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # --- METRICS COLLECTION ---\n",
        "            _, predicted_classes = torch.max(scores, 1)\n",
        "\n",
        "            temp_preds.extend(predicted_classes.cpu().tolist())\n",
        "            temp_labels.extend(y_batch_long.cpu().tolist())\n",
        "\n",
        "            # Print progress every 100 batches\n",
        "            if (i + 1) % 100 == 0:\n",
        "\n",
        "                batch_preds = np.array(temp_preds)\n",
        "                batch_labels = np.array(temp_labels)\n",
        "\n",
        "                # --- CALCULATE ALL METRICS ---\n",
        "                batch_accuracy = accuracy_score(batch_labels, batch_preds)\n",
        "                batch_f1 = f1_score(batch_labels, batch_preds, average='binary', zero_division=0)\n",
        "\n",
        "                # New: Calculate Recall and Precision for the Malicious (positive/Class 1) class\n",
        "                batch_recall = recall_score(batch_labels, batch_preds, average='binary', zero_division=0)\n",
        "                batch_precision = precision_score(batch_labels, batch_preds, average='binary', zero_division=0)\n",
        "\n",
        "                # Print the combined report\n",
        "                avg_loss = running_loss / 100\n",
        "                print(f\"[Epoch {epoch+1}, Batch {i+1}] \"\n",
        "                      f\"Loss: {avg_loss:.4f} | \"\n",
        "                      f\"Acc: {batch_accuracy:.4f} | \"\n",
        "                      f\"F1: {batch_f1:.4f} | \"\n",
        "                      f\"Prec: {batch_precision:.4f} | \"\n",
        "                      f\"Recall: {batch_recall:.4f}\")\n",
        "\n",
        "                # Reset counters for the next 100 batches\n",
        "                running_loss = 0.0\n",
        "                temp_preds, temp_labels = [], []\n",
        "    # ------------------------------------\n",
        "    # 5. Final Model Saving (Updated Logic)\n",
        "    # ------------------------------------\n",
        "\n",
        "    # In DDP, ensure only Rank 0 saves the model\n",
        "    if not USE_DDP or RANK == 0:\n",
        "\n",
        "        # Get the actual model state, unwrapping DDP if necessary\n",
        "        model_to_save = model.module if USE_DDP else model\n",
        "\n",
        "        # Save the final model state to 'model.pkl'\n",
        "        torch.save(model_to_save.state_dict(), OUTPUT_FILENAME)\n",
        "        print(f\"\\nTraining Complete. Final model parameters saved to: {OUTPUT_FILENAME}\")\n",
        "\n",
        "    if USE_DDP:\n",
        "        dist.destroy_process_group()"
      ],
      "metadata": {
        "id": "wbRHxWdbf-uE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lstm_encoder(EPOCHS=2, LSTM_CHECKPOINT_PATH='kkogDANTE'):\n",
        "    # ------------------------------------\n",
        "    # 1. Device and Data Loading Setup\n",
        "    # ------------------------------------\n",
        "    current_device = torch.device(DEVICE)\n",
        "    print(f\"Starting LSTM Encoder training on device {current_device}...\")\n",
        "\n",
        "    # For the Encoder training, we don't need the y_path (malicious labels)\n",
        "    # but the InsiderThreatDataset loads it, so we'll just ignore it in the loop.\n",
        "    dataset = InsiderThreatDataset(X_PATH, Y_PATH)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ------------------------------------\n",
        "    # 2. Model, Loss, and Optimizer\n",
        "    # ------------------------------------\n",
        "\n",
        "    # Initialize the Encoder model and move to device\n",
        "    # Note: LSTM_Encoder is NOT wrapped in the InsiderClassifier here.\n",
        "    model = LSTM_Encoder().to(current_device)\n",
        "    model.train() # Set to training mode for dropout and decoder output\n",
        "\n",
        "    # Loss function for reconstruction (input is categorical, output is LogSoftmax)\n",
        "    # We use NLLLoss combined with F.one_hot() to handle the categorical reconstruction.\n",
        "    criterion = nn.NLLLoss()\n",
        "    optimizer = optim.Adam(model.parameters()) # original one used default lr\n",
        "\n",
        "    # ------------------------------------\n",
        "    # 3. Training Loop\n",
        "    # ------------------------------------\n",
        "    for epoch in range(EPOCHS):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, (X_batch, _) in enumerate(dataloader):\n",
        "            # X_batch is the padded sequence (B, S). Target must be Long tensor.\n",
        "            X_batch = X_batch.to(current_device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass: the encoder returns the reconstructed sequence (LogSoftmax output)\n",
        "            reconstructed_X = model(X_batch)\n",
        "\n",
        "            # --- Calculate Loss (Categorical Reconstruction) ---\n",
        "            # 1. reconstructed_X shape: (B, S, V) [Logits]\n",
        "            # 2. X_batch shape: (B, S) [Target IDs]\n",
        "            # NLLLoss expects (N*V, C) logits and (N*V,) target IDs.\n",
        "\n",
        "            # Reshape logits to (B*S, V) and target to (B*S)\n",
        "            loss = criterion(\n",
        "                reconstructed_X.permute(0, 2, 1), # NLLLoss expects (B, V, S) input\n",
        "                X_batch.long()\n",
        "            )\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} | Avg Reconstruction Loss: {avg_loss:.4f}\", end='\\t')\n",
        "        print()\n",
        "\n",
        "    # ------------------------------------\n",
        "    # 4. Save the Checkpoint\n",
        "    # ------------------------------------\n",
        "    # Save the final state dictionary to the required path\n",
        "    torch.save(model.state_dict(), f'{LSTM_CHECKPOINT_PATH}')\n",
        "    print(f\"\\nLSTM Encoder training complete. Checkpoint saved to: {LSTM_CHECKPOINT_PATH}\")"
      ],
      "metadata": {
        "id": "o0YbGHrof-om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_lstm_encoder()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cF1itrrpG32",
        "outputId": "0a21e81e-485e-4c62-a362-a849b93010a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting LSTM Encoder training on device cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2507181893.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  self.X = torch.tensor(self.X.tolist(), dtype=torch.long)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2 | Avg Reconstruction Loss: 0.2398\t\n",
            "Epoch 2/2 | Avg Reconstruction Loss: 0.0127\t\n",
            "\n",
            "LSTM Encoder training complete. Checkpoint saved to: kkogDANTE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_cnn_classifier()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDJlSddWn-bg",
        "outputId": "64e6ea28-5b56-4e9f-bb57-1983607da4f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Device: Initializing on device cuda\n",
            "Starting unweighted training...\n",
            "[Epoch 1, Batch 100] Loss: 0.3180 | Acc: 0.9881 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 200] Loss: 0.1261 | Acc: 0.9964 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 300] Loss: 0.1124 | Acc: 0.9959 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 400] Loss: 0.0914 | Acc: 0.9980 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 500] Loss: 0.0238 | Acc: 0.9986 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 600] Loss: 0.0393 | Acc: 0.9983 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 700] Loss: 0.0207 | Acc: 0.9980 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 800] Loss: 0.0137 | Acc: 0.9989 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 900] Loss: 0.0198 | Acc: 0.9980 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 1000] Loss: 0.0097 | Acc: 0.9988 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 1100] Loss: 0.0122 | Acc: 0.9983 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 1200] Loss: 0.0152 | Acc: 0.9980 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 1300] Loss: 0.0165 | Acc: 0.9981 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 1400] Loss: 0.0138 | Acc: 0.9981 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 1500] Loss: 0.0096 | Acc: 0.9988 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 1600] Loss: 0.0057 | Acc: 0.9992 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 1700] Loss: 0.0134 | Acc: 0.9980 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 1800] Loss: 0.0145 | Acc: 0.9980 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 1900] Loss: 0.0136 | Acc: 0.9978 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 2000] Loss: 0.0147 | Acc: 0.9977 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 2100] Loss: 0.0118 | Acc: 0.9983 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 2200] Loss: 0.0112 | Acc: 0.9984 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 1, Batch 2300] Loss: 0.0144 | Acc: 0.9978 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 100] Loss: 0.0111 | Acc: 0.9983 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 200] Loss: 0.0159 | Acc: 0.9973 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 300] Loss: 0.0166 | Acc: 0.9972 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 400] Loss: 0.0149 | Acc: 0.9978 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 500] Loss: 0.0116 | Acc: 0.9981 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 600] Loss: 0.0054 | Acc: 0.9992 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 700] Loss: 0.0150 | Acc: 0.9975 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 800] Loss: 0.0105 | Acc: 0.9984 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 900] Loss: 0.0111 | Acc: 0.9983 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 1000] Loss: 0.0076 | Acc: 0.9988 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 1100] Loss: 0.0091 | Acc: 0.9988 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 1200] Loss: 0.0112 | Acc: 0.9983 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 1300] Loss: 0.0126 | Acc: 0.9981 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 1400] Loss: 0.0077 | Acc: 0.9989 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 1500] Loss: 0.0075 | Acc: 0.9989 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 1600] Loss: 0.0125 | Acc: 0.9981 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 1700] Loss: 0.0165 | Acc: 0.9973 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 1800] Loss: 0.0100 | Acc: 0.9984 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 1900] Loss: 0.0093 | Acc: 0.9984 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 2000] Loss: 0.0077 | Acc: 0.9989 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 2100] Loss: 0.0116 | Acc: 0.9980 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 2200] Loss: 0.0095 | Acc: 0.9983 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 2, Batch 2300] Loss: 0.0124 | Acc: 0.9981 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 100] Loss: 0.0138 | Acc: 0.9978 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 200] Loss: 0.0107 | Acc: 0.9984 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 300] Loss: 0.0104 | Acc: 0.9984 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 400] Loss: 0.0113 | Acc: 0.9984 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 500] Loss: 0.0145 | Acc: 0.9980 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 600] Loss: 0.0123 | Acc: 0.9981 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 700] Loss: 0.0069 | Acc: 0.9989 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 800] Loss: 0.0124 | Acc: 0.9981 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 900] Loss: 0.0066 | Acc: 0.9992 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 1000] Loss: 0.0106 | Acc: 0.9983 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 1100] Loss: 0.0196 | Acc: 0.9977 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 1200] Loss: 0.0140 | Acc: 0.9977 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 1300] Loss: 0.0213 | Acc: 0.9970 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 1400] Loss: 0.0083 | Acc: 0.9988 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 1500] Loss: 0.0126 | Acc: 0.9980 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 1600] Loss: 0.0139 | Acc: 0.9975 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 1700] Loss: 0.0085 | Acc: 0.9989 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 1800] Loss: 0.0066 | Acc: 0.9989 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 1900] Loss: 0.0120 | Acc: 0.9983 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 2000] Loss: 0.0109 | Acc: 0.9984 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 2100] Loss: 0.0186 | Acc: 0.9977 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 2200] Loss: 0.0135 | Acc: 0.9981 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 3, Batch 2300] Loss: 0.0132 | Acc: 0.9983 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 4, Batch 100] Loss: 0.0064 | Acc: 0.9994 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 4, Batch 200] Loss: 0.0119 | Acc: 0.9977 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 4, Batch 300] Loss: 0.0236 | Acc: 0.9962 | F1: 0.1429 | Prec: 1.0000 | Recall: 0.0769\n",
            "[Epoch 4, Batch 400] Loss: 0.0112 | Acc: 0.9983 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 4, Batch 500] Loss: 0.0080 | Acc: 0.9989 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 4, Batch 600] Loss: 3.2037 | Acc: 0.9680 | F1: 0.0097 | Prec: 0.0052 | Recall: 0.0714\n",
            "[Epoch 4, Batch 700] Loss: 32.2526 | Acc: 0.9883 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 4, Batch 800] Loss: 15.7017 | Acc: 0.9883 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 4, Batch 900] Loss: 39.4408 | Acc: 0.9983 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 4, Batch 1000] Loss: 22.1801 | Acc: 0.9984 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 4, Batch 1100] Loss: 14.7160 | Acc: 0.9983 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 4, Batch 1200] Loss: 9.7699 | Acc: 0.9970 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 4, Batch 1300] Loss: 1.4882 | Acc: 0.9950 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 4, Batch 1400] Loss: 2.0173 | Acc: 0.9972 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 4, Batch 1500] Loss: 0.9541 | Acc: 0.9980 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 4, Batch 1600] Loss: 0.8155 | Acc: 0.9991 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 4, Batch 1700] Loss: 1.4169 | Acc: 0.9959 | F1: 0.1333 | Prec: 0.1250 | Recall: 0.1429\n",
            "[Epoch 4, Batch 1800] Loss: 1.0974 | Acc: 0.9973 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 4, Batch 1900] Loss: 1.0510 | Acc: 0.9967 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 4, Batch 2000] Loss: 0.6253 | Acc: 0.9973 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 4, Batch 2100] Loss: 1.6098 | Acc: 0.9966 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 4, Batch 2200] Loss: 1.5946 | Acc: 0.9962 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 4, Batch 2300] Loss: 1.3020 | Acc: 0.9967 | F1: 0.0870 | Prec: 0.1111 | Recall: 0.0714\n",
            "[Epoch 5, Batch 100] Loss: 0.9945 | Acc: 0.9970 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 5, Batch 200] Loss: 0.6008 | Acc: 0.9981 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 5, Batch 300] Loss: 0.3589 | Acc: 0.9977 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 5, Batch 400] Loss: 0.4315 | Acc: 0.9975 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 5, Batch 500] Loss: 0.3044 | Acc: 0.9958 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 5, Batch 600] Loss: 0.8394 | Acc: 0.9952 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 5, Batch 700] Loss: 0.4353 | Acc: 0.9969 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 5, Batch 800] Loss: 0.3571 | Acc: 0.9972 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 5, Batch 900] Loss: 1.2663 | Acc: 0.9948 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 5, Batch 1000] Loss: 0.5962 | Acc: 0.9950 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 5, Batch 1100] Loss: 0.4044 | Acc: 0.9945 | F1: 0.0541 | Prec: 0.0556 | Recall: 0.0526\n",
            "[Epoch 5, Batch 1200] Loss: 0.4509 | Acc: 0.9952 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 5, Batch 1300] Loss: 1.5460 | Acc: 0.9988 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 5, Batch 1400] Loss: 0.7837 | Acc: 0.9984 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 5, Batch 1500] Loss: 0.0657 | Acc: 0.9977 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 5, Batch 1600] Loss: 0.0842 | Acc: 0.9970 | F1: 0.0952 | Prec: 0.0909 | Recall: 0.1000\n",
            "[Epoch 5, Batch 1700] Loss: 0.1024 | Acc: 0.9972 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 5, Batch 1800] Loss: 0.0918 | Acc: 0.9967 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 5, Batch 1900] Loss: 0.0665 | Acc: 0.9977 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 5, Batch 2000] Loss: 0.0949 | Acc: 0.9977 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 5, Batch 2100] Loss: 0.2586 | Acc: 0.9944 | F1: 0.0526 | Prec: 0.0500 | Recall: 0.0556\n",
            "[Epoch 5, Batch 2200] Loss: 0.1308 | Acc: 0.9950 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 5, Batch 2300] Loss: 0.1258 | Acc: 0.9964 | F1: 0.0800 | Prec: 0.0909 | Recall: 0.0714\n",
            "[Epoch 6, Batch 100] Loss: 0.2636 | Acc: 0.9958 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 6, Batch 200] Loss: 0.0477 | Acc: 0.9984 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 6, Batch 300] Loss: 0.2398 | Acc: 0.9947 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 6, Batch 400] Loss: 0.1034 | Acc: 0.9952 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 6, Batch 500] Loss: 0.1008 | Acc: 0.9992 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 6, Batch 600] Loss: 0.0736 | Acc: 0.9972 | F1: 0.1000 | Prec: 0.1111 | Recall: 0.0909\n",
            "[Epoch 6, Batch 700] Loss: 0.0563 | Acc: 0.9962 | F1: 0.0769 | Prec: 0.0667 | Recall: 0.0909\n",
            "[Epoch 6, Batch 800] Loss: 0.1413 | Acc: 0.9989 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 6, Batch 900] Loss: 0.1269 | Acc: 0.9958 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 6, Batch 1000] Loss: 0.1409 | Acc: 0.9959 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 6, Batch 1100] Loss: 0.0517 | Acc: 0.9973 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 6, Batch 1200] Loss: 0.0985 | Acc: 0.9962 | F1: 0.1429 | Prec: 0.1429 | Recall: 0.1429\n",
            "[Epoch 6, Batch 1300] Loss: 0.0317 | Acc: 0.9975 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 6, Batch 1400] Loss: 0.0285 | Acc: 0.9972 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 6, Batch 1500] Loss: 0.0450 | Acc: 0.9967 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 6, Batch 1600] Loss: 0.0171 | Acc: 0.9983 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 6, Batch 1700] Loss: 0.0237 | Acc: 0.9969 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 6, Batch 1800] Loss: 0.0381 | Acc: 0.9972 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 6, Batch 1900] Loss: 0.0253 | Acc: 0.9978 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 6, Batch 2000] Loss: 0.0867 | Acc: 0.9941 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 6, Batch 2100] Loss: 0.0390 | Acc: 0.9970 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 6, Batch 2200] Loss: 0.0409 | Acc: 0.9972 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 6, Batch 2300] Loss: 0.0325 | Acc: 0.9977 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 7, Batch 100] Loss: 0.0268 | Acc: 0.9964 | F1: 0.0800 | Prec: 0.1667 | Recall: 0.0526\n",
            "[Epoch 7, Batch 200] Loss: 0.0401 | Acc: 0.9967 | F1: 0.0870 | Prec: 0.1250 | Recall: 0.0667\n",
            "[Epoch 7, Batch 300] Loss: 0.0308 | Acc: 0.9983 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 7, Batch 400] Loss: 0.0247 | Acc: 0.9977 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 7, Batch 500] Loss: 0.0254 | Acc: 0.9972 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 7, Batch 600] Loss: 0.0263 | Acc: 0.9983 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 7, Batch 700] Loss: 0.0407 | Acc: 0.9978 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 7, Batch 800] Loss: 0.0163 | Acc: 0.9988 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 7, Batch 900] Loss: 0.0256 | Acc: 0.9980 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 7, Batch 1000] Loss: 0.0157 | Acc: 0.9980 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 7, Batch 1100] Loss: 0.0341 | Acc: 0.9991 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 7, Batch 1200] Loss: 0.0139 | Acc: 0.9988 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 7, Batch 1300] Loss: 0.0204 | Acc: 0.9977 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 7, Batch 1400] Loss: 0.0106 | Acc: 0.9984 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 7, Batch 1500] Loss: 0.0138 | Acc: 0.9980 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 7, Batch 1600] Loss: 0.0260 | Acc: 0.9973 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 7, Batch 1700] Loss: 0.0204 | Acc: 0.9973 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 7, Batch 1800] Loss: 0.0344 | Acc: 0.9969 | F1: 0.0909 | Prec: 0.2000 | Recall: 0.0588\n",
            "[Epoch 7, Batch 1900] Loss: 0.0162 | Acc: 0.9984 | F1: 0.1667 | Prec: 1.0000 | Recall: 0.0909\n",
            "[Epoch 7, Batch 2000] Loss: 0.0146 | Acc: 0.9973 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 7, Batch 2100] Loss: 0.0182 | Acc: 0.9975 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 7, Batch 2200] Loss: 0.0142 | Acc: 0.9983 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 7, Batch 2300] Loss: 0.0154 | Acc: 0.9980 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 8, Batch 100] Loss: 0.0132 | Acc: 0.9984 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 8, Batch 200] Loss: 0.0177 | Acc: 0.9981 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 8, Batch 300] Loss: 0.0197 | Acc: 0.9972 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 8, Batch 400] Loss: 0.0157 | Acc: 0.9978 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 8, Batch 500] Loss: 0.0193 | Acc: 0.9973 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 8, Batch 600] Loss: 0.0177 | Acc: 0.9977 | F1: 0.1176 | Prec: 0.2500 | Recall: 0.0769\n",
            "[Epoch 8, Batch 700] Loss: 0.0100 | Acc: 0.9989 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 8, Batch 800] Loss: 0.0235 | Acc: 0.9973 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 8, Batch 900] Loss: 3.4922 | Acc: 0.9762 | F1: 0.0256 | Prec: 0.0138 | Recall: 0.1818\n",
            "[Epoch 8, Batch 1000] Loss: 7.3049 | Acc: 0.9891 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 8, Batch 1100] Loss: 14.1110 | Acc: 0.9948 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 8, Batch 1200] Loss: 21.4867 | Acc: 0.9973 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 8, Batch 1300] Loss: 3.0145 | Acc: 0.9972 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 8, Batch 1400] Loss: 0.7943 | Acc: 0.9969 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 8, Batch 1500] Loss: 1.0192 | Acc: 0.9966 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 8, Batch 1600] Loss: 3.7080 | Acc: 0.9956 | F1: 0.0667 | Prec: 0.0556 | Recall: 0.0833\n",
            "[Epoch 8, Batch 1700] Loss: 1.2868 | Acc: 0.9989 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 8, Batch 1800] Loss: 0.7091 | Acc: 0.9964 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 8, Batch 1900] Loss: 2.5242 | Acc: 0.9942 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 8, Batch 2000] Loss: 1.0328 | Acc: 0.9988 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 8, Batch 2100] Loss: 0.9953 | Acc: 0.9969 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 8, Batch 2200] Loss: 0.7021 | Acc: 0.9950 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 8, Batch 2300] Loss: 0.6816 | Acc: 0.9975 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 9, Batch 100] Loss: 0.2328 | Acc: 0.9972 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 9, Batch 200] Loss: 0.1903 | Acc: 0.9962 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 9, Batch 300] Loss: 0.5955 | Acc: 0.9939 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 9, Batch 400] Loss: 1.2078 | Acc: 0.9967 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 9, Batch 500] Loss: 0.2334 | Acc: 0.9964 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 9, Batch 600] Loss: 0.1464 | Acc: 0.9966 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 9, Batch 700] Loss: 0.0926 | Acc: 0.9972 | F1: 0.1000 | Prec: 0.1000 | Recall: 0.1000\n",
            "[Epoch 9, Batch 800] Loss: 0.2066 | Acc: 0.9962 | F1: 0.0769 | Prec: 0.0833 | Recall: 0.0714\n",
            "[Epoch 9, Batch 900] Loss: 0.1979 | Acc: 0.9964 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 9, Batch 1000] Loss: 0.1352 | Acc: 0.9977 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 9, Batch 1100] Loss: 0.1636 | Acc: 0.9978 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 9, Batch 1200] Loss: 0.1000 | Acc: 0.9978 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 9, Batch 1300] Loss: 0.3055 | Acc: 0.9969 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 9, Batch 1400] Loss: 0.1252 | Acc: 0.9984 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 9, Batch 1500] Loss: 0.2404 | Acc: 0.9941 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 9, Batch 1600] Loss: 0.5689 | Acc: 0.9966 | F1: 0.0833 | Prec: 0.1111 | Recall: 0.0667\n",
            "[Epoch 9, Batch 1700] Loss: 0.1763 | Acc: 0.9973 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 9, Batch 1800] Loss: 0.1128 | Acc: 0.9967 | F1: 0.0870 | Prec: 0.1000 | Recall: 0.0769\n",
            "[Epoch 9, Batch 1900] Loss: 0.2347 | Acc: 0.9952 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 9, Batch 2000] Loss: 0.2998 | Acc: 0.9984 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 9, Batch 2100] Loss: 0.1302 | Acc: 0.9975 | F1: 0.1111 | Prec: 0.1000 | Recall: 0.1250\n",
            "[Epoch 9, Batch 2200] Loss: 0.0768 | Acc: 0.9973 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 9, Batch 2300] Loss: 0.0609 | Acc: 0.9964 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 10, Batch 100] Loss: 0.3193 | Acc: 0.9948 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 10, Batch 200] Loss: 0.0694 | Acc: 0.9967 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 10, Batch 300] Loss: 0.0322 | Acc: 0.9975 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 10, Batch 400] Loss: 0.0457 | Acc: 0.9978 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 10, Batch 500] Loss: 0.0820 | Acc: 0.9953 | F1: 0.0625 | Prec: 0.0526 | Recall: 0.0769\n",
            "[Epoch 10, Batch 600] Loss: 0.2281 | Acc: 0.9983 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 10, Batch 700] Loss: 0.0474 | Acc: 0.9980 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 10, Batch 800] Loss: 0.0930 | Acc: 0.9962 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 10, Batch 900] Loss: 0.0436 | Acc: 0.9972 | F1: 0.1818 | Prec: 0.2222 | Recall: 0.1538\n",
            "[Epoch 10, Batch 1000] Loss: 0.0219 | Acc: 0.9977 | F1: 0.1176 | Prec: 0.1667 | Recall: 0.0909\n",
            "[Epoch 10, Batch 1100] Loss: 0.0188 | Acc: 0.9984 | F1: 0.2857 | Prec: 0.5000 | Recall: 0.2000\n",
            "[Epoch 10, Batch 1200] Loss: 0.0171 | Acc: 0.9978 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 10, Batch 1300] Loss: 0.0233 | Acc: 0.9983 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 10, Batch 1400] Loss: 0.0484 | Acc: 0.9969 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 10, Batch 1500] Loss: 0.0336 | Acc: 0.9975 | F1: 0.1111 | Prec: 0.2000 | Recall: 0.0769\n",
            "[Epoch 10, Batch 1600] Loss: 0.0835 | Acc: 0.9972 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 10, Batch 1700] Loss: 2.0853 | Acc: 0.9706 | F1: 0.0105 | Prec: 0.0056 | Recall: 0.0909\n",
            "[Epoch 10, Batch 1800] Loss: 2.3818 | Acc: 0.9984 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 10, Batch 1900] Loss: 0.3523 | Acc: 0.9967 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 10, Batch 2000] Loss: 1.7580 | Acc: 0.9956 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 10, Batch 2100] Loss: 0.4165 | Acc: 0.9978 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 10, Batch 2200] Loss: 0.2666 | Acc: 0.9969 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "[Epoch 10, Batch 2300] Loss: 0.1033 | Acc: 0.9978 | F1: 0.0000 | Prec: 0.0000 | Recall: 0.0000\n",
            "\n",
            "Training Complete. Final model parameters saved to: ogDANTE_logsoft.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hfrZcdXcn_tz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}